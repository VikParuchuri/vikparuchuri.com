---
layout: post
title: "Evolve your own beats: automatically generating music via algorithms"
date: 2013-07-26 8:28
comments: true
categories:
    - music
    - audio
    - sound
    - R
    - python
    - remix
---

I recently went to an excellent [music meetup](http://www.meetup.com/Boston-Music-Technology-Group/) where people spoke about the intersection of music and technology.  One [speaker in particular](http://dadabots.tumblr.com/) talked about how music is now being generated by computer.

Music has always fascinated me.  It can make us feel emotions in a way few media can.  Sadly, I have always been unable to play an instrument well.  Generating music by computer lets me leverage one of my strengths, computer programming (which, contrary to popular belief, can be extremely creative), in order to make music.  Although I'm not exactly sure how people are doing it now (explicit parsing rules?), I thought of a way to do it algorithmically (because everything is more fun with algorithms, right?)

I opted to pursue a strategy that "evolves" music out of other pieces of music.  I chose this strategy in order to emphasize the process.  Seeing a track take shape is very exciting, and you can go back to its history and experience each of the tracks that took part in its creation.

I'm going to broadly outline the keys to my strategy below. You might think that a lot of my points are crazy, so bear with me for a while(at least until I can prove them out later):

* We can easily acquire music that is already categorized (ie labelled as classical/techno/electronic, etc)
* We can teach a computer to categorize new music automatically.
* Teaching a computer to categorize music automatically will give us a musical quality assessment tool (patent pending on the MQAT!)
* Once we have an assessment tool, we can generate music, and then use the assessment tool to see if it is any good
* Profit?

One important thing to note is that while the music itself is automatically generated, the building blocks of the final song are extracted from existing songs and mixed together.  So don't worry about this replacing humans anytime.  The key to the algorithm is actually finding readily available, free music, and I would sincerely like to thank [last.fm](http://www.last.fm/music/+free-music-downloads) and the wonderful artists who put up their music up in their free downloads section.  I am not sure how much my algorithm has obfuscated the original sounds of the music, but if anyone recognizes theirs, I would love to hear from them.

<a name="player"></a>

Here are some samples of the computer generated music.  Because of the way they are generated, they all have short riffs that repeat.  I want to try to improve this behavior in the future, but I will let your judge what is here now for yourself.  Caution:  Listening to these may give you a headache.  If the player below does not show up you may have to visit [my site](http://www.vikparuchuri.com/blog/evolve-your-own-beats-automatically-generating-music#player) to see it.

<div>
    <div id="jquery_jplayer_1" class="jp-jplayer"></div>
    <div id="jp_container_1">
      <div class="jp-playlist">
        <ul>
          <li></li>
        </ul>
      </div>
       <div class="jp-type-single">
      <div class="jp-gui jp-interface">
        <ul class="jp-controls">
          <li><a href="javascript:;" class="jp-play" tabindex="1">play</a></li>
          <li><a href="javascript:;" class="jp-pause" tabindex="1">pause</a></li>
          <li><a href="javascript:;" class="jp-stop" tabindex="1">stop</a></li>
          <li><a href="javascript:;" class="jp-mute" tabindex="1" title="mute">mute</a></li>
          <li><a href="javascript:;" class="jp-unmute" tabindex="1" title="unmute">unmute</a></li>
          <li><a href="javascript:;" class="jp-volume-max" tabindex="1" title="max volume">max volume</a></li>
        </ul>
        <div class="jp-progress">
          <div class="jp-seek-bar">
            <div class="jp-play-bar"></div>
          </div>
        </div>
        <div class="jp-volume-bar">
          <div class="jp-volume-bar-value"></div>
        </div>
        <div class="jp-time-holder">
          <div class="jp-current-time"></div>
          <div class="jp-duration"></div>
          <ul class="jp-toggles">
            <li><a href="javascript:;" class="jp-repeat" tabindex="1" title="repeat">repeat</a></li>
            <li><a href="javascript:;" class="jp-repeat-off" tabindex="1" title="repeat off">repeat off</a></li>
          </ul>
        </div>
      </div>
      <div class="jp-title">
        <ul>
          <li>Bubble</li>
        </ul>
      </div>
      <div class="jp-no-solution">
        <span>Update Required</span>
        To play the media you will need to either update your browser to a recent version or update your <a href="http://get.adobe.com/flashplayer/" target="_blank">Flash plugin</a>.
      </div>
    </div>
    </div>
      <script type="text/javascript">
    $(document).ready(function(){

        var myPlaylist = new jPlayerPlaylist({
          jPlayer: "#jquery_jplayer_1",
          cssSelectorAncestor: "#jp_container_1"
        },
        [
          {
            title: "Heavy",
            oga:"http://www.vikparuchuri.com/downloads/code/07-25-2013-223535.ogg"
          },
          {
            title: "Cheerful",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-224817.ogg"
          },
          {
            title: "Atmospheric",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-224913.ogg"
          },
          {
            title: "Dark",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-225227.ogg"
          },
          {
            title: "Melancholy",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-225539.ogg"
          },
          {
            title: "Interrupted Piano",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-230321.ogg"
          },
          {
            title: "The Duel",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-230511.ogg"
          },
          {
            title: "The Duel Pt.2",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-230801.ogg"
          },
          {
            title: "Excitement",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-231644.ogg"
          },
          {
            title: "Darkly Soothing",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-231949.ogg"
          },
          {
            title: "Where are we going?",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-232912.ogg"
          },
          {
            title: "Atmosphere",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-233221.ogg"
          },
          {
            title: "Interrupted Peace",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-233529.ogg"
          },
          {
            title: "Movement",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-233835.ogg"
          },
          {
            title: "Electronic",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-234141.ogg"
          },
          {
            title: "The Duel Pt. 3",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-234449.ogg"
          },
          {
            title: "Peace",
            oga: "http://www.vikparuchuri.com/downloads/code/07-25-2013-234755.ogg"
          },
          {
            title: "Tranquil",
            oga: "http://www.vikparuchuri.com/downloads/code/07-26-2013-000022.ogg"
          }
        ],
        {
          playlistOptions: {
            enableRemoveControls: true
          },
          swfPath: "/javascripts",
          supplied: "oga",
          smoothPlayBar: true,
          keyEnabled: true,
          audioFullScreen: true
        });
        });
  </script>
</div>

<!--more-->

So, how did you do it?
-----------------------------------------------------

I used some of the knowledge that I had built up from doing [audio analysis](http://www.vikparuchuri.com/blog/analyzing-audio-to-figure-out-which-simpsons-character-is-speaking/) on the Simpsons to attempt music generation.  If you have time, I recommend reading that entry.

The first thing I did was write a [web spider](http://en.wikipedia.org/wiki/Web_crawler) to automatically grab links to free music from the last.fm free music section.  I was able to download 500 tracks using this crawler.  Half of the tracks were classical music, and half electronic.

Unfortunately, these tracks were in .mp3 format.  [Mp3](http://en.wikipedia.org/wiki/MP3)
 is a proprietary codec, which makes it harder to decode, process, and work with.  I was able to get around this by converting the .mp3 tracks to [ogg](https://en.wikipedia.org/wiki/Ogg), which is a free audio format, using the tools mpg123 and oggenc.

I was then able to extract audio features from these tracks.  Audio features are numbers that describe the sound in a song.  Sound is just a wave.  We can measure how instense that wave is at various points in time, which gives us a sequence of numbers that describe that sound.  We can later use those numbers to replay a song, which is what happens in modern music players.

Here are the sound waves in the first 10 seconds of a classical track:

![10 seconds of song](http://www.vikparuchuri.com/images/evolve-beats/song_10s.png)

And here are the sound waves in the first 10 seconds of an electronic track:

![10 seconds of song](http://www.vikparuchuri.com/images/evolve-beats/esong_10s.png)

One of the lines is the left channel audio, and one is the right.  It doesn't matter overly much which is which.  Let's say that green is right.  We can quickly see that the second track has more intense peaks, and appears to have higher energy when compared with the classical track.  Using our knowledge of classical music vs electronic music, this makes sense.  Just like we can look at the two graphs and understand that they are different, so can a computer.

For example, when we read the classical track in, the first 10 audio samples produce the following:

{%math%}
\begin{bmatrix}
2.35185598e-05 & -1.04448336e-05\\
-3.46823663e-06 & -3.73403673e-05\\
-2.69492170e-06 & -1.44758296e-05\\
9.47549870e-06 & 2.09419904e-05\\
-2.70856035e-05 & 3.44590421e-06\\
-3.01332675e-05 & 2.74870854e-05\\
-1.44664727e-06 & 7.49632018e-05\\
-3.80197125e-05 & 2.56412422e-05\\
-5.61815832e-05 & -1.29676855e-05\\
-4.73532873e-06 & 3.69851950e-05

\end{bmatrix}
{%endmath%}

And the first 10 audio samples from the second track produce:

{%math%}
\begin{bmatrix}
.18460009e-05 & -6.85636178e-06\\
-1.83847499e-06 & -3.18044404e-05\\
-5.80170308e-06 & -1.55047346e-05\\
3.17729814e-06 &  2.17720844e-05\\
-2.59620665e-05 &  8.40834218e-06\\
-2.97414826e-05 &  2.96072376e-05\\
-7.36495986e-06 &  7.55500150e-05\\
-3.90941568e-05 &  2.75659913e-05\\
-5.52387464e-05 & -1.12897151e-05\\
-6.22146581e-06 &  4.04318343e-05
\end{bmatrix}
{%endmath%}

These numbers describe how the sound waves look.  The numbers on the left are the left audio track, and the numbers on the right are the right audio track.  Each sound is sampled `44100` times per second, so one second of audio will result in a `44100x2` matrix.  The `10x2` matrices that we see above represent approximately `.00022` seconds of audio.

Let's say that someone asked you to look at the numbers from a 1 minute audio sample and summarize what made it good or bad.  There will be `2646000` numbers in each audio track, and it is almost impossible to make any meaningful decisions when swamped with so much information.  It is the same way with a learning algorithm, so we extract audio features to reduce the dimensionality and make it easier for the computer to understand what is going on.

We calculate several features, including changes in peak intensity throughout the track, [zero crossing rate](http://en.wikipedia.org/wiki/Zero-crossing_rate), and [mel frequency cepstrum coefficients](http://en.wikipedia.org/wiki/Mel-frequency_cepstrum).  All of the features can be found [here](https://github.com/VikParuchuri/evolve-music).  These features describe each of our sound tracks using only `319` numbers instead of `2646000`+.

Training the algorithm
--------------------------------------

Once we have our features for each of our tracks, we can go ahead and create our musical quality assessment tool (MQAT).  Our MQAT will decide if a track is classical or electronic music.  We will have to assign the label of "0" to one type of music, and "1" to the other.  I gave classical a 1, and electronic a 0, which is not a reflection of my own musical taste.

With the features and labels, it is possible to train an algorithm that will make up the heart of the MQAT.  The algorithm can tell if an input track is classical or electronic.  More crucially, it will give you a decimal number between 0 and 1 that indicates how classical or how electronic a track is.  We can take this number, and detect the quality of music by how close it is to 0 or 1.  If a piece of music is a classified close to a 0 or a 1, its audio signature is likely to match our existing tracks, which will make it more likely to be good than if it doesn't.

Splicing/Remixing
-----------------------------------------

Once we have a musical quality assessment tool, we can splice new tracks into our existing tracks to make remixes.  Here is roughly how splicing works:

![splicing workflow](http://www.vikparuchuri.com/images/evolve-beats/workflow.png)

So, we pull a randomly selected short audio clip out of a randomly selected track (Track 2).  We then use the quality assessor (Algorithm) to see if it is good (if it close to 0 or 1, as we are defining things that are pure examples as "good").  We do this a few times, and take the "best" audio clip (Best Clip), and insert it into several evenly spaced places in the original track (Track 1).

This splicing procedure is repeated until the original song (the song that clips are being spliced into, Track 1) gets a high quality rating and is different from all of the other tracks (measured through euclidean distance), or until the specified number of splices is done.  The high quality rating is important, because we want the new song to sound decent.  The distance is also important; we don't just want to make a derivative of one of our existing songs, we want to make something a bit original (although I can't exactly quantify how original).

Through this splicing, we end up with a very different track from the original; one that could have insertions from dozens of other songs, making a very intricate remix (indeed, in almost all cases, there are no traces of the original song).

Looking at a single song
------------------------------------------

Once we have a way to splice, we can perform the splicing for each of our input songs.

Once we get our first remixed song, we can look at the first 10 seconds:

![10 seconds of song](http://www.vikparuchuri.com/images/evolve-beats/mix_10s.png)

We can see that its audio signature appears to be more towards the electronic side, rather than the classical side.

And here is the history of that song:

<div>
<table border="1" class="dataframe table display">
<thead> 
<tr><th>song_index</th><th>iteration</th><th>quality</th><th>distance</th><th>splice_song_index</th><th>splice_song</th></tr>
</thead>
<tbody>
<tr><td>13</td><td>-1</td><td>0.905635225885</td><td>0</td><td>0</td><td>N/A</td></tr>
<tr><td>13</td><td>0</td><td>0.905635225885</td><td>5.53740630788e-18</td><td>75</td><td>25D1%2581%25D1%258F.ogg</td></tr>
<tr><td>13</td><td>10</td><td>0.143886317386</td><td>0.0875533808843</td><td>177</td><td>Wenn%2Bsie%2Blacht.ogg</td></tr>
<tr><td>13</td><td>20</td><td>0.331367817368</td><td>0.334819780512</td><td>122</td><td>Sprinkler.ogg</td></tr>
<tr><td>13</td><td>30</td><td>0.0819852832353</td><td>0.0989308766371</td><td>331</td><td>Generalized%2BVegetal%2B%257E%2BTheme%2Bof%2BStar%2BGeneral.ogg</td></tr>
<tr><td>13</td><td>40</td><td>0.876639434639</td><td>0.252705284634</td><td>340</td><td>crystal%2Bviolet.ogg</td></tr>
<tr><td>13</td><td>50</td><td>0.817946248196</td><td>0.400284251004</td><td>25</td><td>Bullseye.ogg</td></tr>
<tr><td>13</td><td>60</td><td>0.0499835257335</td><td>0.0218079019239</td><td>345</td><td>Messiah%253A%2BHalleluia%2BChorus%2B%2528Handel%2529.ogg</td></tr>
<tr><td>13</td><td>70</td><td>0.952422919673</td><td>0.437933963581</td><td>478</td><td>Etude%2Bin%2BDb%2BMajor%252C%2BPosthumous%2B%2528Chopin%2529.ogg</td></tr>
</tbody>
</table>

  <script>
    $('.table').dataTable({
        "bPaginate": false,
        "bLengthChange": true,
        "bSort": false,
        "bStateSave": true,
        "sScrollY": 300,
        "sScrollX": 500,
        "aLengthMenu": [[50, 100, -1], [50, 100, "All"]],
        "iDisplayLength": 6,
    });
    </script><br/>
</div>

The `song_index` is the number of the original song in my song database.  `iteration` is which iteration the algorithm is on (the algorithm will iterate a maximum of 100 times, but can stop sooner if it makes a good song).  At each iteration, a best clip will be selected (as per the diagram above), and splicing will be performed.  `quality` is the quality of the track from 0 to 1.  0 means the track is electronic, and 1 means that it is classical, and numbers in between indicate various degrees of electronic-ness and classical-ness.  The algorithm considers a created song to be good if its quality is close to 0 or close to 1.  `distance` is how similar the created song is to the existing songs in the database.  The algorithm will try to create a high quality song that is not similar to the songs in the database.  `splice_song_index` is the index of the the song that is being spliced into the created song at the specified iteration.  `splice_song` is the name of the spliced song.

In the above history, we start at iteration -1, which is the original song.  The first splice song, `25D1%2581%25D1%258F.ogg`, is then chosen to be spliced in.  Best clips are selected and spliced into the original track 10 times, after which a new song is picked on iteration 10.  The quality is also computed at iteration 10, and we can see that the song has moved from classical to electronic quite dramatically.  We continue this process until iteration 70, when the quality is very high, and the distance between this track and all the others is also high, making this a good song to save.

It is pretty amazing to see music take shape, and having the history helps us better understand and experience the evolution of the track.

Looking at all songs
-------------------------------------------

Once we have generated a few tracks, we can reduce the features in them to `2` instead of `319` using [singular value decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition).  We can also do this for the original songs in our database.  This projects the values into two dimensions, which allows us to plot them:

![song similarity](http://www.vikparuchuri.com/images/evolve-beats/music_types.png)

We are visualizing 522 songs, 22 of which are generated songs.  The dots closer together are more similar.  We can see that classical and electronic tend to be grouped together, with some overlap.

What is very surprising is that all of our generated tracks are distinct from classical and electronic music, but they are all very similar to each other.  We have evolved an entirely new genre of music out of classical and electronic! (or perhaps two different genres, given the clustering).  Does anyone want to name this?

If we altered some of the parameters of our algorithm that control splicing and timing, we may be able to create more genres of music.

Final Words/Improvements
------------------------------------------

This was an interesting project to work on, particularly because there really isn't a "right" answer with music (at least until humans listen to something and decide if they like it or not).  I can't stress how much of a debt I owe to the human creators who made the source music.  This algorithm is less about usurping the existing creative process than trying to define a new and distinct creative process.

You can find all the code for everything I have described so far [here](https://github.com/VikParuchuri/evolve-music).

The music made here isn't the best music you will ever hear.  Here are some ways that I can think of to make this better:

* Make transitions between segments more smooth.
* Train an additional algorithm to recognize notes that can be combined well (or just use a simple heuristic to see if one can blend into another)
* Use more input tracks for training
* Try this with other types of music, like rock or hip hop
* Try to filter out specific bands, such as vocals, and combine them
* Splice in irregular patterns
* Add in operations other than splicing
* Try to optimize the fitness of the output track (more of a genetic algorithm approach)

I will probably fiddle with the algorithm and try some of these in the next few days, if I have time.

I hope you enjoyed this, and would love to hear your thoughts.